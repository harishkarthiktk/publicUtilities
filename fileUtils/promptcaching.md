# Context Caching: Essply while using RooCode/Cline or OpenCode

## How It Works

### 1. **Model Capability** (Provider Side)
Context caching is only available on certain models:

**‚úÖ Supports Caching:**
- **Anthropic Claude**: Sonnet 4.5, Opus 4, Sonnet 3.5, Haiku 3.5
  - Uses "Prompt Caching" feature
- **OpenAI GPT-4**: Turbo, GPT-4o
  - Uses "Prompt Caching" (automatic in some cases)
- **Google Gemini**: 1.5 Pro, 1.5 Flash

**‚ùå No Caching:**
- GPT-3.5 Turbo
- Most Grok models (currently)
- Older model versions

### 2. **Agent/Tool Configuration** (Client Side)

For tools like **Roo-Cline** and **OpenCode**, here's how to ensure caching is used:

---

## Roo-Cline Configuration

Roo-Cline (the VS Code extension) typically handles caching automatically, but you should verify:

### Check Settings:
1. Open VS Code Settings (`Ctrl/Cmd + ,`)
2. Search for "Roo-Cline" or "Cline"
3. Look for:
   ```
   Cline: API Configuration
   ‚îî‚îÄ‚îÄ Prompt Caching: [Enabled/Disabled]
   ```

### Via API Configuration:
If using Anthropic's API directly in Roo-Cline:
```json
{
  "anthropic": {
    "apiKey": "your-key",
    "model": "claude-sonnet-4-20250514",
    "enablePromptCaching": true  // ‚Üê Ensure this is true
  }
}
```

### How to Verify Caching is Working:
- Check the **token usage** in responses
- Anthropic API responses include:
  ```json
  {
    "usage": {
      "input_tokens": 1000,
      "cache_creation_input_tokens": 5000,  // First request
      "cache_read_input_tokens": 5000,      // Subsequent requests
      "output_tokens": 500
    }
  }
  ```

---

## OpenCode (Aider/OpenAI Codex)

For OpenAI-based tools:

### Automatic Caching (GPT-4 Turbo/4o):
OpenAI has **automatic prompt caching** for repeated context, but you can optimize:

```python
# In your OpenAI API calls
from openai import OpenAI

client = OpenAI(api_key="your-key")

response = client.chat.completions.create(
    model="gpt-4-turbo-2024-04-09",  # or gpt-4o
    messages=[
        {
            "role": "system",
            "content": "Your system prompt here..."  # This gets cached
        },
        {
            "role": "user",
            "content": "New query here"
        }
    ],
    # OpenAI caches automatically for repeated prefixes
)
```

### Manual Configuration (if using Aider):
```bash
# In your .aider.conf.yml
model: gpt-4-turbo
cache-prompts: true  # Enable caching
```

---

## Best Practices for Context Caching

### 1. **Structure Your Prompts for Caching**
Put **static context first**, **dynamic queries last**:

```
‚úÖ GOOD (Cacheable):
[System Prompt] ‚Üê Cached
[Full Codebase Context] ‚Üê Cached
[Previous Conversation] ‚Üê Cached
[New User Query] ‚Üê Not cached (changes each time)

‚ùå BAD (Not Cacheable):
[New User Query]
[Full Codebase Context] ‚Üê Can't cache if prefix changes
```

### 2. **Use File-Based Context**
For Roo-Cline/Aider, use **@file** or **@folder** to maintain consistent context:
```
# This maintains cacheable context
/add src/
/ask How can I optimize this function?
```

### 3. **Check Minimum Cache Size**
- **Anthropic**: Minimum 1,024 tokens to cache
- **OpenAI**: Minimum 1,000 tokens

---

## How to Verify Caching in Your Tools

### For Roo-Cline:
1. Check the **output panel** in VS Code
2. Look for token usage breakdown
3. You should see `cache_read_input_tokens` in subsequent requests

### For OpenAI/Aider:
1. Enable verbose mode: `aider --verbose`
2. Check the logs for repeated context
3. Monitor your OpenAI dashboard for reduced input token costs

### General Test:
Run the **same query twice** with the same context:
- **First request**: Full input token cost
- **Second request**: ~90% reduction on cached portions

---

## Updated Cost Estimator Note

Your cost estimator script correctly assumes:
- Caching **only works** if the model supports it
- **First iteration**: Full cost
- **Subsequent iterations**: 90% reduction on project context

This matches how Anthropic and OpenAI implement caching!

---

**TL;DR**: 
- ‚úÖ Use models that support caching (Claude Sonnet 4.5, GPT-4 Turbo)
- ‚úÖ Ensure your tool/agent has caching enabled in settings
- ‚úÖ Structure prompts with static context first
- ‚úÖ Verify by checking token usage in API responses

Excellent question! Let me explain how **prefix-based caching** works, which is crucial to understanding context caching.

## How Context Caching Works: The "Prefix" Concept

Context caching uses **prefix matching** - the AI provider caches the **beginning portion** of your prompt that stays the same across requests.

### Visual Example:

#### ‚ùå BAD: Prefix Changes (Can't Cache)

**Request 1:**
```
[New Query: "Fix the login bug"] ‚Üê Changes every time
[Codebase: 50,000 tokens]
```

**Request 2:**
```
[New Query: "Add error handling"] ‚Üê Different prefix!
[Codebase: 50,000 tokens]
```

**Problem**: The prompt starts differently each time, so the provider can't identify what's "the same" to cache.

---

#### ‚úÖ GOOD: Prefix Stays Same (Cacheable)

**Request 1:**
```
[System: "You are a coding assistant"]  ‚Üê Same
[Codebase: 50,000 tokens]                ‚Üê Same
[Conversation History: Previous Q&A]      ‚Üê Grows but prefix stays
[New Query: "Fix the login bug"]          ‚Üê Only this changes
```

**Request 2:**
```
[System: "You are a coding assistant"]  ‚Üê Same (CACHED ‚úì)
[Codebase: 50,000 tokens]                ‚Üê Same (CACHED ‚úì)
[Conversation History: Previous Q&A]      ‚Üê Same prefix (CACHED ‚úì)
[New Query: "Add error handling"]         ‚Üê New, not cached
```

**Why it works**: The provider sees "the first 51,000 tokens are identical to last request" and uses the cache.

---

## The Technical Details

### How Providers Match Prefixes:

When you send a request, the provider:
1. Compares your prompt to recent requests **from the start**
2. Finds the **longest matching prefix**
3. Loads that prefix from cache
4. Only processes the **new/changed portion**

### Example Breakdown:

```
Request 1:
Token 0-1000:    System prompt
Token 1001-51000: Codebase
Token 51001-52000: "Fix bug X"

Request 2:
Token 0-1000:    System prompt         ‚Üê Matches! Cache HIT
Token 1001-51000: Codebase              ‚Üê Matches! Cache HIT
Token 51001-52000: "Add feature Y"      ‚Üê Different, process normally
```

If you had put the query first:
```
Request 1:
Token 0-1000:    "Fix bug X"            ‚Üê Different in Request 2!
Token 1001-2000:  System prompt         ‚Üê Can't match, cache MISS
Token 2001-52000: Codebase              ‚Üê Can't match, cache MISS
```

---

## Real-World Scenarios

### Scenario 1: Changing Timestamps (Cache Breaker)

‚ùå **This breaks caching:**
```python
messages = [
    {"role": "system", "content": f"Time: {datetime.now()} - You are a coding assistant"},
    {"role": "user", "content": codebase},
    {"role": "user", "content": query}
]
```
Every request has a different timestamp at the **start**, breaking the prefix match.

‚úÖ **This maintains caching:**
```python
messages = [
    {"role": "system", "content": "You are a coding assistant"},
    {"role": "user", "content": codebase},
    {"role": "user", "content": f"{query} (Time: {datetime.now()})"}
]
```
Timestamp is at the **end**, so prefix stays the same.

---

### Scenario 2: Growing Conversation History

‚úÖ **This works well:**
```
Request 1:
[System] [Codebase] [Q1] ‚Üê All cached for next request

Request 2:
[System] [Codebase] [Q1] [A1] [Q2] ‚Üê Q1+A1 cached, only Q2 processed

Request 3:
[System] [Codebase] [Q1] [A1] [Q2] [A2] [Q3] ‚Üê Q1+A1+Q2+A2 cached
```

The conversation grows at the **end**, so the growing prefix stays cacheable.

---

### Scenario 3: Roo-Cline's Approach

Roo-Cline typically structures it like this:

```
[System Prompt: Tool usage instructions]     ‚Üê Static, cacheable
[Codebase Context: @workspace files]         ‚Üê Static per session
[Conversation History: Turn 1, 2, 3...]      ‚Üê Grows but prefix stable
[Current Task: User's latest request]        ‚Üê Only new part
```

Each request adds to the conversation history, but the prefix (system + codebase) remains the same, so it gets cached.

---

## Why "Prefix" Matters

Think of it like **bookmarks in a book**:

- The AI provider says: "I've read pages 1-100 before (prefix), I'll remember those."
- If your next request is "pages 1-100 + page 101", they only read page 101.
- But if your next request is "page 0 (new intro) + pages 1-100", they have to re-read everything because page 0 changed the beginning.

---

## Anthropic's Specific Implementation

Anthropic uses **cache breakpoints** that you can control:

```python
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "System instructions...",
                "cache_control": {"type": "ephemeral"}  # Mark this for caching
            }
        ]
    },
    {
        "role": "user", 
        "content": [
            {
                "type": "text",
                "text": "Full codebase...",
                "cache_control": {"type": "ephemeral"}  # Cache this too
            }
        ]
    },
    {
        "role": "user",
        "content": "Current query"  # Not cached
    }
]
```

The `cache_control` markers tell Anthropic: "Cache up to this point."

---

## Summary Table

| Prompt Structure | Cacheable? | Why |
|-----------------|------------|-----|
| `[Query] [Codebase]` | ‚ùå No | Query changes, prefix changes |
| `[Codebase] [Query]` | ‚úÖ Yes | Codebase stays at start |
| `[Timestamp] [System] [Code]` | ‚ùå No | Timestamp changes prefix |
| `[System] [Code] [Timestamp]` | ‚úÖ Yes | Timestamp at end |
| `[System] [Code] [History] [Query]` | ‚úÖ Yes | Only query changes |
| `[Random] [System] [Code]` | ‚ùå No | Random breaks prefix |

---

**TL;DR**: 
- "Prefix" = the **beginning** of your prompt
- Caching only works if the **start** of your prompt stays **identical** across requests
- Put **static content first**, **changing content last**
- Think: "Can the AI say 'I've seen tokens 0-50,000 before'?" If yes, it caches!

Excellent question! Yes, **vector indexing with Qdrant in Roo-Cline can significantly reduce token counts**, but it works differently than context caching. Let me break down how they work together:

## Vector Indexing vs Context Caching: Different Approaches

### **Context Caching** (What we discussed earlier)
- **When**: You send the SAME large context repeatedly
- **How**: Provider caches the identical prefix
- **Savings**: ~90% on repeated context
- **Limitation**: Still need to send ALL relevant code initially

### **Vector Indexing/RAG** (Qdrant approach)
- **When**: You have a LARGE codebase but only need PARTS of it
- **How**: Semantic search finds relevant code chunks
- **Savings**: 70-95% reduction by sending ONLY relevant code
- **Benefit**: Works even on first request

---

## How Qdrant + Roo-Cline Works

### Traditional Approach (No Vector DB):
```
User: "Fix the authentication bug"

Roo-Cline sends to AI:
‚îú‚îÄ üìÅ src/auth/           (5,000 tokens)
‚îú‚îÄ üìÅ src/database/       (8,000 tokens)
‚îú‚îÄ üìÅ src/api/            (12,000 tokens)
‚îú‚îÄ üìÅ src/utils/          (6,000 tokens)
‚îú‚îÄ üìÅ src/frontend/       (15,000 tokens)
‚îî‚îÄ üìÅ tests/              (5,000 tokens)
TOTAL: 51,000 tokens ‚Üí High cost
```

### With Qdrant Vector Indexing:
```
User: "Fix the authentication bug"

1. Qdrant searches for relevant code:
   - Semantic search: "authentication", "login", "auth"
   - Returns top-k most relevant chunks

2. Roo-Cline sends ONLY relevant code:
   ‚îú‚îÄ üìÑ src/auth/login.py       (800 tokens)
   ‚îú‚îÄ üìÑ src/auth/jwt_handler.py (600 tokens)
   ‚îú‚îÄ üìÑ src/api/auth_routes.py  (500 tokens)
   ‚îî‚îÄ üìÑ tests/test_auth.py      (400 tokens)
   TOTAL: 2,300 tokens ‚Üí 95% reduction!
```

---

## Combining Both: Maximum Savings

Here's the powerful combination:

### Scenario: Multiple refactoring iterations on authentication

#### **First Request** (Vector Indexing Only):
```
Query: "Fix login validation"
Qdrant retrieves: 2,300 relevant tokens
Cost: 2,300 tokens input (no cache yet)
Savings: 95% vs sending full codebase
```

#### **Second Request** (Vector Indexing + Caching):
```
Query: "Add 2FA to login"
Qdrant retrieves: Same 2,300 tokens (similar topic)
Cost: ~230 tokens (90% cached!)
Savings: 99.5% vs full codebase without caching
```

#### **Third Request** (Different area):
```
Query: "Optimize database queries"
Qdrant retrieves: NEW 3,500 tokens (database code)
Cost: 3,500 tokens (new context, no cache)
But still: 93% savings vs full codebase
```

---

## Configuration in Roo-Cline

### Enable Qdrant Vector Indexing:

1. **Install Qdrant** (local or cloud):
```bash
# Local Docker
docker run -p 6333:6333 qdrant/qdrant

# Or use Qdrant Cloud
```

2. **Configure Roo-Cline settings** (`settings.json`):
```json
{
  "roo-cline.vectorStore": {
    "enabled": true,
    "provider": "qdrant",
    "qdrant": {
      "url": "http://localhost:6333",
      "collectionName": "my-codebase",
      "embeddingModel": "text-embedding-3-small" // OpenAI
    }
  },
  "roo-cline.contextStrategy": {
    "maxTokens": 50000,
    "useVectorSearch": true,
    "topK": 10,  // Number of relevant chunks to retrieve
    "hybridSearch": true  // Combine with file path matching
  }
}
```

3. **Index your codebase**:
```
Command Palette ‚Üí Roo-Cline: Index Workspace
```

---

## Real-World Token Savings Example

### Your 51,004 Token Project:

#### **Without Vector Indexing** (20% refactoring):
```
Analysis Phase:    51,004 tokens √ó 2 iterations = 102,008
Planning Phase:    51,004 tokens √ó 3 iterations = 153,012
Execution Phase:   51,004 tokens √ó 10 iterations = 510,040
Review Phase:      51,004 tokens √ó 4 iterations = 204,016
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL INPUT: 969,076 tokens

With caching (90% reduction after first):
First iteration: 51,004 tokens
Remaining 18 iterations: 51,004 √ó 0.1 √ó 18 = 91,807
TOTAL: 142,811 tokens
```

#### **With Vector Indexing + Caching**:
```
Each query retrieves ~5,000 relevant tokens (10% of codebase)

Analysis Phase:    5,000 √ó 2 = 10,000
Planning Phase:    5,000 √ó 3 = 15,000  
Execution Phase:   5,000 √ó 10 = 50,000
Review Phase:      5,000 √ó 4 = 20,000
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL INPUT (without caching): 95,000 tokens

With caching on vector results:
First: 5,000, Remaining 18: 5,000 √ó 0.1 √ó 18 = 9,000
TOTAL: 14,000 tokens

SAVINGS: 969,076 ‚Üí 14,000 = 98.6% reduction!
```

---

## When Vector Indexing Helps Most

### ‚úÖ **Best Use Cases:**
1. **Large codebases** (>20,000 tokens)
2. **Localized changes** (working on specific features)
3. **Exploratory refactoring** (not sure where the issue is)
4. **Multi-domain projects** (frontend + backend + ML)

### ‚ö†Ô∏è **Less Effective For:**
1. **Small projects** (<10,000 tokens) - overhead not worth it
2. **Architectural changes** (need to see everything)
3. **Cross-cutting concerns** (logging, error handling across all files)
4. **Initial full analysis** (need complete picture)

---

## Hybrid Strategy: Best of Both Worlds

Roo-Cline can use a **hybrid approach**:

```
Query: "Refactor authentication"

Step 1: Vector search finds relevant files
‚îú‚îÄ auth/login.py (high relevance: 0.95)
‚îú‚îÄ api/auth_routes.py (high relevance: 0.89)
‚îî‚îÄ config/security.py (medium relevance: 0.72)

Step 2: Also include by file path
‚îú‚îÄ auth/__init__.py (always include entry points)
‚îî‚îÄ tests/test_auth.py (always include related tests)

Step 3: Expand context if needed
If AI responds "need to see database models":
‚îî‚îÄ Fetch models/user.py on next iteration

Result: Start small (5k tokens), grow as needed
```

---

## Cost Comparison Updated

Let me update your cost estimator scenario:

### **51,004 tokens, 20% refactoring, Sonnet 4.5:**

| Strategy | Total Input Tokens | Cost |
|----------|-------------------|------|
| **No optimization** | 969,076 | $2.91 |
| **Caching only** | 142,811 | $0.43 |
| **Vector indexing only** | 95,000 | $0.29 |
| **Vector + Caching** | **14,000** | **$0.04** |

**98.6% cost reduction with both techniques!**

---

## How to Verify Vector Indexing is Working

### Check Roo-Cline Output:
```
‚úì Vector search retrieved 8 relevant files (4,200 tokens)
‚úì Using cached context from previous query
üìä Token usage: 420 input (90% cached), 500 output
```

### Monitor Qdrant Dashboard:
```bash
# Access Qdrant UI
http://localhost:6333/dashboard

# Check:
- Collection size
- Query latency
- Search results quality
```

---

## Setup Checklist for Maximum Savings

- [ ] Enable context caching in API settings
- [ ] Install and configure Qdrant
- [ ] Index your codebase with vector embeddings
- [ ] Set appropriate `topK` value (5-15 chunks)
- [ ] Use hybrid search (vector + keyword)
- [ ] Structure prompts with static prefix first
- [ ] Monitor token usage per request
- [ ] Adjust retrieval settings based on query types

---

## TL;DR

**Vector Indexing (Qdrant):**
- Reduces tokens by retrieving ONLY relevant code (70-95% savings)
- Works from first request
- Best for large, modular codebases

**Context Caching:**
- Reduces tokens by reusing SAME context (90% savings)
- Works from second request onwards
- Best for iterative work on same area

**Together:**
- **98%+ token reduction**
- **Massive cost savings** ($2.91 ‚Üí $0.04 in your example)
- **Faster responses** (less to process)
- **Smarter context** (only what's needed)

Excellent! Let me give you practical examples of prompt structuring for both refactoring and feature implementation scenarios.

## Example 1: Adding Authentication to an Existing API

### ‚ùå BAD Structure (Breaks Cache)
```
Messages:
[
  {
    "role": "user",
    "content": "I need to add JWT authentication to the /api/users endpoint"
  },
  {
    "role": "system", 
    "content": "You are an expert backend developer..."
  },
  {
    "role": "user",
    "content": "<codebase files>"
  }
]
```
**Problem**: User query comes FIRST, so every new request changes the prefix.

---

### ‚úÖ GOOD Structure (Cacheable)
```
Messages:
[
  {
    "role": "system",
    "content": "You are an expert backend developer specializing in secure API development. You follow OWASP best practices and write clean, maintainable code."
  },
  {
    "role": "user",
    "content": "=== PROJECT CONTEXT ===
    
    <codebase>
    // src/api/routes.py (500 lines)
    // src/models/user.py (200 lines)
    // src/config/settings.py (100 lines)
    </codebase>
    
    === CURRENT TASK ===
    Add JWT authentication to the /api/users endpoint"
  }
]
```

**Next iteration:**
```
Messages:
[
  {
    "role": "system",
    "content": "You are an expert backend developer..." // ‚Üê CACHED
  },
  {
    "role": "user",
    "content": "=== PROJECT CONTEXT ===       // ‚Üê CACHED
    <codebase>...</codebase>                // ‚Üê CACHED
    
    === CURRENT TASK ===
    Now add refresh token functionality"    // ‚Üê Only this is new
  }
]
```

---

## Example 2: Refactoring Legacy Code

### ‚ùå BAD Structure
```
Iteration 1:
"Refactor the UserService class to use dependency injection"
[System prompt]
[Codebase]

Iteration 2:
"Add unit tests for the refactored UserService"  // ‚Üê Prefix changed!
[System prompt]
[Codebase]
```

---

### ‚úÖ GOOD Structure
```
=== STATIC PREFIX (Always at the top) ===

ROLE: Senior Software Engineer - Code Refactoring Specialist

PROJECT OVERVIEW:
- Language: Python 3.11
- Framework: FastAPI
- Database: PostgreSQL
- Testing: pytest
- Current Tech Debt: High coupling, no dependency injection

REFACTORING GUIDELINES:
1. Maintain backward compatibility
2. Add comprehensive tests
3. Follow SOLID principles
4. Update documentation
5. Preserve existing API contracts

=== CODEBASE CONTEXT ===

File: src/services/user_service.py
[Full file content - 800 lines]

File: src/models/user.py
[Full file content - 200 lines]

File: tests/test_user_service.py
[Full file content - 300 lines]

=== REFACTORING HISTORY ===
[Empty on first request, grows with each iteration]

=== CURRENT REFACTORING TASK ===
Phase 1: Refactor UserService to use dependency injection
```

**Iteration 2 (adds to the end):**
```
=== STATIC PREFIX ===                    // ‚Üê CACHED
[Same as above...]

=== REFACTORING HISTORY ===              // ‚Üê CACHED
Phase 1 completed: Added DI container, refactored UserService

=== CURRENT REFACTORING TASK ===         // ‚Üê Only this is new
Phase 2: Add comprehensive unit tests for refactored UserService
```

---

## Example 3: Implementing New Feature (E-commerce Cart)

### ‚úÖ GOOD Structure
```
=== SYSTEM ROLE ===
You are a full-stack developer building an e-commerce platform.
Stack: React + TypeScript (frontend), Node.js + Express (backend)
Database: MongoDB
State Management: Redux Toolkit

=== PROJECT ARCHITECTURE ===

Frontend Structure:
/src
  /components
  /features
  /store
  /services
  /types

Backend Structure:
/api
  /routes
  /controllers
  /models
  /middleware
  /services

=== EXISTING CODEBASE ===

// Frontend - Product Component
[Full component code]

// Frontend - Redux Store Setup
[Full store code]

// Backend - Product Routes
[Full routes code]

// Backend - Product Controller
[Full controller code]

// Backend - Product Model
[Full model code]

=== FEATURE REQUIREMENTS ===
Feature: Shopping Cart System

Requirements:
1. Add items to cart
2. Update quantities
3. Remove items
4. Calculate totals with tax
5. Persist cart across sessions
6. Guest cart support

=== IMPLEMENTATION PHASES ===

=== CURRENT PHASE ===
Phase 1: Create Cart MongoDB model and basic CRUD operations
```

**Next iteration:**
```
[All static prefix above stays the same - CACHED]

=== IMPLEMENTATION PHASES ===
‚úÖ Phase 1: Cart model created with schema validation

=== CURRENT PHASE ===
Phase 2: Build Express API routes for cart operations
```

---

## Example 4: Bug Fix with Context

### ‚úÖ GOOD Structure
```
=== DEBUGGING SESSION ===
Date: 2025-10-10
Issue ID: BUG-1234
Priority: High
Reporter: QA Team

=== SYSTEM INFORMATION ===
Environment: Production
Node Version: 20.10.0
Database: PostgreSQL 15
Error Rate: 15% of login attempts

=== RELEVANT CODEBASE ===

File: src/auth/login.controller.ts
[Full file - 400 lines]

File: src/auth/jwt.service.ts  
[Full file - 250 lines]

File: src/middleware/auth.middleware.ts
[Full file - 150 lines]

File: config/database.ts
[Full file - 100 lines]

=== ERROR LOGS ===
[Last 50 error entries]

=== REPRODUCTION STEPS ===
1. User logs in with valid credentials
2. JWT token generated successfully
3. Subsequent API calls fail with 401
4. Token appears to be invalidated prematurely

=== DEBUGGING HISTORY ===
[Empty initially]

=== CURRENT INVESTIGATION ===
Step 1: Analyze JWT token generation logic for expiration issues
```

**After finding the issue:**
```
[All static context above - CACHED]

=== DEBUGGING HISTORY ===
‚úÖ Found: Token expiration set to 15min but refresh logic expects 60min
‚úÖ Root cause: Mismatch between token TTL and refresh window

=== CURRENT INVESTIGATION ===
Step 2: Implement fix - align token TTL with refresh logic and add tests
```

---

## Example 5: Database Migration

### ‚úÖ GOOD Structure
```
=== MIGRATION CONTEXT ===

Database: PostgreSQL 14 ‚Üí 15
ORM: Prisma
Migration Strategy: Zero-downtime rolling migration
Backup Status: ‚úÖ Completed (2025-10-10 10:30 UTC)

=== SCHEMA OVERVIEW ===

Current Schema (v1.2.3):
[Full Prisma schema - 500 lines]

Migration Requirements:
1. Add user_preferences table
2. Add indexes on email lookups
3. Modify user.role enum (add 'moderator')
4. Add user.last_login timestamp
5. Maintain backward compatibility

=== EXISTING MIGRATIONS ===
[List of previous 10 migrations]

=== AFFECTED CODEBASE ===

File: prisma/schema.prisma
[Current schema]

File: src/models/user.model.ts
[User model code]

File: src/repositories/user.repository.ts  
[Repository code]

=== MIGRATION HISTORY ===
[Empty initially]

=== CURRENT MIGRATION TASK ===
Create migration: Add user_preferences table with foreign key to users
```

---

## Example 6: API Endpoint Refactoring (Microservices)

### ‚úÖ GOOD Structure
```
=== SERVICE CONTEXT ===

Service: User Management Service
Architecture: Microservices (Event-driven)
Message Broker: RabbitMQ
API Gateway: Kong
Service Mesh: Istio

=== CURRENT SERVICE TOPOLOGY ===

user-service (this service)
  ‚Üì depends on ‚Üì
auth-service (JWT validation)
notification-service (email/SMS)
analytics-service (event tracking)

=== SERVICE CODEBASE ===

File: src/controllers/user.controller.ts
[Full controller - 600 lines]

File: src/services/user.service.ts
[Full service layer - 800 lines]

File: src/events/user.events.ts
[Event publishers - 200 lines]

File: src/dtos/user.dto.ts
[Data transfer objects - 150 lines]

=== API CONTRACTS ===

REST Endpoints:
- GET /api/v1/users/:id
- POST /api/v1/users
- PUT /api/v1/users/:id
- DELETE /api/v1/users/:id

Event Topics:
- user.created
- user.updated
- user.deleted

=== REFACTORING GOALS ===
1. Extract email logic to notification-service
2. Move analytics to event subscribers
3. Reduce controller complexity
4. Improve error handling
5. Add request validation

=== REFACTORING PROGRESS ===
[Empty initially]

=== CURRENT REFACTORING STEP ===
Step 1: Extract email sending logic and publish user.created events
```

---

## Example 7: Frontend Component Refactoring (React)

### ‚úÖ GOOD Structure  
```
=== FRONTEND REFACTORING SESSION ===

Framework: React 18 + TypeScript
State: Redux Toolkit + RTK Query
Styling: Tailwind CSS + shadcn/ui
Testing: Vitest + React Testing Library

=== COMPONENT HIERARCHY ===

App
‚îî‚îÄ Dashboard
   ‚îú‚îÄ UserProfile (target for refactoring)
   ‚îÇ  ‚îú‚îÄ ProfileHeader
   ‚îÇ  ‚îú‚îÄ ProfileStats
   ‚îÇ  ‚îî‚îÄ ProfileSettings (750 lines - too large!)
   ‚îî‚îÄ Sidebar

=== CURRENT COMPONENTS ===

File: src/components/UserProfile/ProfileSettings.tsx
[Full component - 750 lines, multiple concerns]

File: src/components/UserProfile/types.ts
[Type definitions]

File: src/store/slices/userSlice.ts
[Redux slice]

File: src/hooks/useUserProfile.ts
[Custom hooks]

=== STYLE GUIDE ===
- Components < 300 lines
- Single Responsibility Principle
- Extract business logic to hooks
- Prop drilling max depth: 2 levels
- Use composition over inheritance

=== REFACTORING PLAN ===
Break ProfileSettings into:
1. AccountSettings (email, password)
2. PrivacySettings (visibility, blocking)
3. NotificationSettings (email, push, SMS)
4. PreferenceSettings (theme, language)

=== REFACTORING LOG ===
[Empty initially]

=== CURRENT TASK ===
Extract AccountSettings component with form validation
```

---

## Example 8: Performance Optimization

### ‚úÖ GOOD Structure
```
=== PERFORMANCE OPTIMIZATION SESSION ===

Target: Search API endpoint
Current Performance: 2.5s average response time
Goal: < 500ms response time
Load: 1000 req/min peak

=== PERFORMANCE PROFILE ===

Bottlenecks Identified:
1. Database query: 1.8s (72%)
2. Serialization: 0.5s (20%)
3. Network: 0.2s (8%)

Tools Used:
- New Relic APM
- PostgreSQL EXPLAIN ANALYZE
- Chrome DevTools

=== RELEVANT CODE ===

File: src/api/search.controller.ts
[Controller code - 300 lines]

File: src/services/search.service.ts
[Service with DB queries - 500 lines]

File: src/database/queries/search.queries.ts
[Raw SQL queries - 200 lines]

=== DATABASE SCHEMA ===
[Relevant tables and indexes]

=== OPTIMIZATION HISTORY ===
[Empty initially]

=== CURRENT OPTIMIZATION ===
Optimize product search query with proper indexes and query restructuring
```

---

## Key Pattern Summary

### Structure Template:
```
1. ‚ïê‚ïê‚ïê ROLE/CONTEXT ‚ïê‚ïê‚ïê          (Static - defines AI's role)
   ‚îî‚îÄ Never changes

2. ‚ïê‚ïê‚ïê PROJECT INFO ‚ïê‚ïê‚ïê           (Static - architecture, stack)
   ‚îî‚îÄ Rarely changes

3. ‚ïê‚ïê‚ïê CODEBASE ‚ïê‚ïê‚ïê               (Semi-static - updated occasionally)
   ‚îî‚îÄ Files relevant to task

4. ‚ïê‚ïê‚ïê STANDARDS/GUIDELINES ‚ïê‚ïê‚ïê   (Static - coding standards)
   ‚îî‚îÄ Never changes

5. ‚ïê‚ïê‚ïê HISTORY LOG ‚ïê‚ïê‚ïê            (Growing - but prefix stable)
   ‚îî‚îÄ Append-only, grows at the end

6. ‚ïê‚ïê‚ïê CURRENT TASK ‚ïê‚ïê‚ïê           (Dynamic - only this changes)
   ‚îî‚îÄ Each iteration changes only this section
```

---

## Pro Tips

### 1. **Use Delimiter Sections**
```
=== SECTION NAME ===
[content]

=== NEXT SECTION ===
[content]
```
Makes it easy for both AI and caching system to identify boundaries.

### 2. **Append, Don't Prepend**
```
‚úÖ Good:
[History]
- Task 1 done
- Task 2 done
- Task 3 (current)  ‚Üê Add at end

‚ùå Bad:
[History]
- Task 3 (current)  ‚Üê Adding at start breaks cache
- Task 2 done
- Task 1 done
```

### 3. **Use Timestamps at the End**
```
‚úÖ Good:
=== CURRENT TASK ===
Fix login bug
[Timestamp: 2025-10-10 14:30]

‚ùå Bad:
[Timestamp: 2025-10-10 14:30]
=== CURRENT TASK ===
Fix login bug
```

### 4. **Keep Static Sections Truly Static**
Don't update version numbers or dates in static sections unless absolutely necessary.

---

Want me to create a **prompt template generator** script that structures prompts automatically following these patterns? üéØ


**Excellent observation!** Yes, you're absolutely right - Roo-Cline (and similar AI coding agents) **DO automatically structure prompts** for optimal caching. Let me clarify what happens behind the scenes:

---

## What Roo-Cline Handles Automatically

### ‚úÖ **Automatic Prompt Structuring**

Roo-Cline's internal prompt structure (you don't see this):

```
[System Prompt - Static]
‚îú‚îÄ Role definition
‚îú‚îÄ Tool capabilities
‚îú‚îÄ Output format instructions
‚îî‚îÄ Safety guidelines

[Workspace Context - Semi-static]
‚îú‚îÄ Project structure
‚îú‚îÄ File contents (from @workspace or vector search)
‚îî‚îÄ Configuration files

[Conversation History - Growing]
‚îú‚îÄ Previous user messages
‚îú‚îÄ Previous assistant responses
‚îî‚îÄ Tool execution results

[Current User Input - Dynamic]
‚îî‚îÄ Your latest message
```

This structure is **already optimized** for caching because:
1. Static content stays at the top
2. Context doesn't change unless you modify files
3. History grows at the end (prefix-stable)
4. Only your new query changes

---

## What YOU Control vs What Roo-Cline Controls

### **Roo-Cline's Responsibility (Automatic):**

| What | How It Works |
|------|--------------|
| **Prompt structure** | Always: System ‚Üí Context ‚Üí History ‚Üí Query |
| **Context inclusion** | Decides which files to include based on relevance |
| **Conversation tracking** | Maintains history in cacheable format |
| **Tool execution** | Formats tool results for caching |
| **API calls** | Sends requests with proper cache headers |

### **Your Responsibility (Manual Optimization):**

| What | How You Optimize |
|------|------------------|
| **File selection** | Use `@file` or `@folder` to control context size |
| **Query specificity** | Clear, focused queries = better vector retrieval |
| **Session management** | Keep related work in same session for cache benefit |
| **Vector DB config** | Set appropriate `topK` for retrieval size |

---

## Roo-Cline's Operating Modes & Caching

### **Mode 1: Code (Default)**
```
Automatic behavior:
‚îú‚îÄ Loads workspace context
‚îú‚îÄ Uses vector search if enabled
‚îú‚îÄ Maintains conversation history
‚îî‚îÄ Caches system prompt + workspace

You do nothing - it's optimized!
```

### **Mode 2: Architect**
```
Automatic behavior:
‚îú‚îÄ Broader context (more files)
‚îú‚îÄ Emphasizes structure/patterns
‚îú‚îÄ May load entire directory trees
‚îî‚îÄ Still caches properly

You might: Use @folder to limit scope
```

### **Mode 3: Ask (Chat)**
```
Automatic behavior:
‚îú‚îÄ Minimal code context
‚îú‚îÄ Focuses on knowledge/explanation
‚îú‚îÄ Lighter prompts
‚îî‚îÄ Less caching benefit (smaller context)

Less relevant for caching optimization
```

---

## Where Manual Optimization DOES Help

Even though Roo-Cline handles prompt structure, you can still optimize:

### **1. Context Size Management**

**‚ùå Suboptimal (Automatic):**
```
You: "Fix the login bug"

Roo-Cline automatically includes:
‚îú‚îÄ Entire src/auth/ folder (10 files, 8,000 tokens)
‚îú‚îÄ Related tests (5 files, 3,000 tokens)  
‚îî‚îÄ Config files (3 files, 1,000 tokens)
Total: 12,000 tokens
```

**‚úÖ Optimized (Manual):**
```
You: "Fix the login bug in @src/auth/login.ts"

Roo-Cline includes:
‚îú‚îÄ Only login.ts (1,200 tokens)
‚îú‚îÄ Directly imported files (800 tokens)
‚îî‚îÄ Related test (400 tokens)
Total: 2,400 tokens (80% reduction!)
```

### **2. Session Continuity**

**‚ùå Suboptimal:**
```
Session 1: "Add authentication"
[Cache built: system + workspace]

Close IDE, reopen later

Session 2: "Add password reset"
[Cache LOST - starts fresh]
```

**‚úÖ Optimized:**
```
Session 1: "Add authentication"
[Cache built]

Continue same session:
"Add password reset"
[Cache HIT - 90% savings]

"Add 2FA"
[Cache HIT again]

Keep session alive for related work!
```

### **3. Vector DB Configuration**

**Default Roo-Cline:**
```json
{
  "topK": 15,  // Retrieves 15 chunks
  "chunkSize": 1000  // 1000 tokens per chunk
}
Result: ~15,000 tokens per query
```

**Optimized:**
```json
{
  "topK": 5,  // Retrieves only 5 most relevant
  "chunkSize": 800,
  "minRelevanceScore": 0.7  // Higher quality threshold
}
Result: ~4,000 tokens per query (73% reduction)
```

---

## Real Example: What Roo-Cline Actually Sends

### **Your Input:**
```
You: "Refactor the UserService to use dependency injection"
```

### **What Roo-Cline Sends to Claude (simplified):**

**First Request:**
```
[SYSTEM - 2,000 tokens - CACHED on next request]
You are Roo-Cline, an AI coding assistant...
Available tools: read_file, write_file, list_directory...
Output format: Use <thinking> tags...

[WORKSPACE CONTEXT - 15,000 tokens - CACHED on next request]
Project: /home/user/my-app
Structure:
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ services/
‚îÇ  ‚îÇ  ‚îî‚îÄ user.service.ts (selected by vector search)
‚îÇ  ‚îú‚îÄ models/
‚îÇ  ‚îÇ  ‚îî‚îÄ user.model.ts (related file)
...
[Full file contents]

[CONVERSATION - Empty on first request]

[USER QUERY - 50 tokens - NOT CACHED]
Refactor the UserService to use dependency injection
```

**Second Request:**
```
[SYSTEM - 2,000 tokens]  ‚Üê 90% CACHED
[WORKSPACE - 15,000 tokens]  ‚Üê 90% CACHED
[CONVERSATION - 3,000 tokens]  ‚Üê NEW (previous exchange)
[USER QUERY - 60 tokens]  ‚Üê NEW
"Now add unit tests for the refactored service"

Effective tokens: 200 + 1,500 + 3,000 + 60 = 4,760
Instead of: 2,000 + 15,000 + 3,000 + 60 = 20,060
Savings: 76%!
```

---

## When Manual Prompt Structuring IS Needed

### **Scenario 1: Using Roo-Cline API Directly**
If you're building custom integrations:
```python
# You manually construct prompts
client.messages.create(
    model="claude-sonnet-4-20250514",
    messages=[
        {"role": "system", "content": "..."},  # Your responsibility
        {"role": "user", "content": "..."}     # Structure matters here
    ]
)
```

### **Scenario 2: Custom Workflows/Scripts**
When automating with Roo-Cline's API:
```javascript
// Your script calling Roo-Cline
const result = await rooCline.executeTask({
    task: "Refactor authentication",
    context: buildContext(),  // You control this
    history: loadHistory()     // You control this
});
```

### **Scenario 3: Multi-Agent Systems**
Orchestrating multiple AI agents:
```
Agent 1 (Architect) ‚Üí Designs structure
Agent 2 (Coder) ‚Üí Implements code
Agent 3 (Tester) ‚Üí Writes tests

You manage prompt flow between agents
```

---

## So When Do Those Examples Apply?

The examples I gave earlier are useful for:

### ‚úÖ **When to apply manual structuring:**
1. **Custom AI integrations** (not using Roo-Cline)
2. **Direct API usage** (Claude API, OpenAI API)
3. **Building your own agents/tools**
4. **Understanding** how Roo-Cline works internally
5. **Debugging** cache misses
6. **Optimizing** context selection manually

### ‚ùå **When NOT needed:**
1. **Normal Roo-Cline usage** - it's automatic!
2. **Standard refactoring** - defaults are good
3. **Simple queries** - Roo-Cline handles it

---

## What You SHOULD Focus On

Instead of prompt structure, optimize these:

### **1. Query Clarity**
```
‚ùå "Make it better"
‚úÖ "Refactor UserService to use constructor injection for database dependency"
```

### **2. Context Hints**
```
‚ùå "Fix the bug"
‚úÖ "Fix the authentication bug in @src/auth/login.ts related to JWT expiration"
```

### **3. Incremental Tasks**
```
‚ùå "Refactor entire authentication system"
‚úÖ "Step 1: Extract JWT logic to separate service"
   "Step 2: Add dependency injection"
   "Step 3: Write unit tests"
```

### **4. File Targeting**
```
‚ùå Let Roo-Cline auto-include everything
‚úÖ Use @file and @folder to control scope
```

---

## Verification: Is Roo-Cline Caching?

### **Check the Output Panel:**
```
Roo-Cline Output:
‚îú‚îÄ API Request
‚îÇ  ‚îú‚îÄ Input tokens: 1,234
‚îÇ  ‚îú‚îÄ Cache creation: 15,000  ‚Üê First request
‚îÇ  ‚îî‚îÄ Output tokens: 500
‚îÇ
‚îú‚îÄ API Request (2nd)
‚îÇ  ‚îú‚îÄ Input tokens: 234
‚îÇ  ‚îú‚îÄ Cache read: 15,000  ‚Üê Using cache!
‚îÇ  ‚îî‚îÄ Output tokens: 600
```

### **Look for:**
- `cache_creation_input_tokens` ‚Üí Building cache
- `cache_read_input_tokens` ‚Üí Using cache
- Lower `input_tokens` on subsequent requests

---

## TL;DR

**Your original question is spot-on!**

| Aspect | Roo-Cline (Automatic) | You (Manual) |
|--------|----------------------|--------------|
| Prompt structure | ‚úÖ Optimized | ‚ùå Not needed |
| Cache management | ‚úÖ Handled | ‚ùå Not needed |
| Context caching | ‚úÖ Automatic | ‚ùå Not needed |
| Context selection | ‚ö†Ô∏è Auto (broad) | ‚úÖ Can optimize with @file |
| Query specificity | ‚ùå Can't control | ‚úÖ Your job |
| Session continuity | ‚ö†Ô∏è Until close | ‚úÖ Keep sessions alive |
| Vector DB tuning | ‚ö†Ô∏è Default settings | ‚úÖ Adjust topK/threshold |

**Bottom line:** 
- Roo-Cline handles 90% of optimization automatically
- Focus your effort on: clear queries, file targeting, and session management
- Manual prompt structuring is only needed for custom integrations

Those examples I showed are for **understanding the internals** and **building custom tools**, not for day-to-day Roo-Cline usage!